{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375499b0-83b7-45f5-b95b-badf2d8a6a63",
   "metadata": {},
   "source": [
    "## NER and Preprocessing\n",
    "This notebook preprocesses the articles from the LexisNexis API and performs NER for getting other companies mentioned together with the NAATBatt listed ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c4afb-d3d0-4e9b-b230-312066f44811",
   "metadata": {},
   "source": [
    "### Functions Required\n",
    "Here, also the Stanza model is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b95c19d-589c-4ec7-8e25-8059ae493c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 02:52:34 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765c707872e54ed1a5543a8af4ac7367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 02:52:34 INFO: Downloaded file to /home/h11922823/stanza_resources/resources.json\n",
      "2024-07-27 02:52:34 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-07-27 02:52:35 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-07-27 02:52:35 INFO: Using device: cuda\n",
      "2024-07-27 02:52:35 INFO: Loading: tokenize\n",
      "2024-07-27 02:52:35 INFO: Loading: mwt\n",
      "2024-07-27 02:52:35 INFO: Loading: ner\n",
      "2024-07-27 02:52:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import stanza\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "myLangModel= stanza.Pipeline(lang='en',processors='tokenize,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274670d8-4e82-42b4-af1b-052c5a67a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(entity):\n",
    "    '''\n",
    "        This function removes from the NERs the company endings.\n",
    "        It takes as input a company name and returns it cleaned from its suffix.\n",
    "    '''\n",
    "    entity = entity.replace('Inc','')\n",
    "    entity = entity.replace(\"'s\",'')            \n",
    "    entity = entity.replace('INC','') \n",
    "    entity = entity.replace('AG','')\n",
    "    entity = entity.replace('SA','')\n",
    "    entity = entity.replace('Corp','')\n",
    "    entity = entity.replace('BWA','')\n",
    "    entity = entity.replace('GmBH','')\n",
    "    entity = entity.replace('LLC','')\n",
    "    entity = entity.replace('Group','')\n",
    "    entity = entity.replace('Co ','')\n",
    "    entity = entity.replace('Ltd','')  \n",
    "    entity = entity.replace('LP','')\n",
    "    entity = entity.replace('.','')   \n",
    "    entity = entity.replace(',','')\n",
    "\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ca5381-e171-4faf-ba30-02fbd31b0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values(entity, initial_company):\n",
    "    '''\n",
    "        The purpose of this funciton is to remove unwanted NERs from the output\n",
    "        This function takes as input a NER and the NAATBatt listed company. It \n",
    "        compares the NER with a list of exclusions. If all checks are passed, \n",
    "        it returns true, otherwise false.\n",
    "    '''   \n",
    "\n",
    "    #lists of exclusions\n",
    "    full_terms = ['EV', 'European Comission', 'SEC', 'WEC', 'Europe', 'European Union', 'United States', 'Gigafactory', 'US Army', 'Q1', 'Q2', 'R&D', 'R & D', 'LFP', 'Green Deal']\n",
    "    noise_filter = ['show', 'news', 'retirement', 'market', 'wealth', 'advisors', 'assets under', '2024', '2023', '2022', 'press releases', 'sector', 'stock dashboard', 'nyse', 'oem', 'pension', 'pensional', 'free report', 'investment board', 'council', 'government', 'ukri', 'university', 'trojan', 'the ev industry', 'AnalysisEurope']\n",
    "\n",
    "    #checks if the NER is the NAATBatt listed company\n",
    "    if initial_company in entity.lower():\n",
    "        return False\n",
    "\n",
    "    #checks if a str from noise_filters is present in the name\n",
    "    for term in noise_filter:\n",
    "        if term in entity.lower():\n",
    "            return False\n",
    "    \n",
    "    #checks if a value from full_terms is equal with the NER\n",
    "    for term in full_terms:\n",
    "        if entity == term:\n",
    "            return False\n",
    "\n",
    "    #if all checks are passed return true\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33603fca-a6b5-4d41-9bbb-bd90bc37f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_func (initial_company, entities):    \n",
    "    '''\n",
    "        This function takes as input NERs for one company listed in\n",
    "        the NAATBatt database and preprocesses them. It selects only\n",
    "        organisations and applies the cleaning and filter_values from\n",
    "        above\n",
    "    '''\n",
    "    #keeping only the ORG in the list - not products, cities, countries etc..\n",
    "    entities = [entity for entity in entities if entity[1] == \"ORG\"]\n",
    "    \n",
    "    #keeping only the name of the entities and not other parameters\n",
    "    entities = [entity[0] for entity in entities]\n",
    "\n",
    "    #cleaning the string\n",
    "    entities = [cleaning(entity) for entity in entities]\n",
    "\n",
    "    entities = list(filter(lambda entity: filter_values(entity, initial_company), entities))\n",
    "\n",
    "    #keeping unique results\n",
    "    entities = list(set(entities))\n",
    " \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9aeb7f5-5835-4edb-9a5e-4bd8cbd5bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(stanza_output):\n",
    "    import re\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "\n",
    "    for sentence in stanza_output.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            ner_tag = token.ner\n",
    "            if ner_tag == 'O':\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_type))\n",
    "                    current_entity = []\n",
    "                    current_type = None\n",
    "            else:\n",
    "                entity_type = ner_tag.split('-')[1]\n",
    "                if current_type and current_type != entity_type:\n",
    "                    entities.append((' '.join(current_entity), current_type))\n",
    "                    current_entity = []\n",
    "                current_entity.append(token.text)\n",
    "                current_type = entity_type\n",
    "        \n",
    "        # Append the last entity in the sentence if it exists\n",
    "        if current_entity:\n",
    "            current_entity = ' '.join(current_entity)\n",
    "            tuples = (current_entity, current_type)\n",
    "            \n",
    "            entities.append(tuples)\n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4caa1b5b-3a58-4315-9194-6ea7aa0025a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_companies (initial_company, data):\n",
    "    result_text = []\n",
    "    to_drop = []\n",
    "    for index,string in enumerate(data.sentences):\n",
    "        #for logging\n",
    "        if index % 500 == 0:\n",
    "            print(index)\n",
    "            #clear GPU memory\n",
    "            gc.collect()\n",
    "            torch.cuda.set_device(1)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.set_device(0)\n",
    "            torch.cuda.empty_cache()\n",
    "        string = string.replace('\"\"', '\"')\n",
    "        \n",
    "        if string != '{}' and string is not np.nan:\n",
    "            try:\n",
    "                dicty = json.loads(string)\n",
    "            except json.JSONDecodeError as e:\n",
    "                to_drop.append(index) #will be dropped as the data is corrupted\n",
    "                continue  # Skip to the next string\n",
    "            \n",
    "            pairing = []\n",
    "            \n",
    "            for key in dicty.keys():\n",
    "                text = dicty[key]\n",
    "    \n",
    "                res_copy = myLangModel(text)\n",
    "    \n",
    "                entities = extract_entities(res_copy)\n",
    "    \n",
    "                entities = preprocess_func(\n",
    "                                        initial_company = initial_company.lower(),\n",
    "                                        entities = entities)\n",
    "    \n",
    "                company_pair = [(entity, initial_company) for entity in entities]\n",
    "                \n",
    "                pairing.append((text, company_pair))\n",
    "            \n",
    "            result_text.append(pairing)\n",
    "\n",
    "        else: \n",
    "            to_drop.append(index) #if sentences is empty, nothing we can do\n",
    "            \n",
    "    \n",
    "    \n",
    "    data = data.drop(to_drop, axis =0)\n",
    "    \n",
    "    print(initial_company, to_drop, len(result_text), '\\n', '\\n')\n",
    "    \n",
    "    data['prompt_prep'] = result_text\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adf1036d-2496-453f-8779-013600deed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_companies_explosion (data):\n",
    "    #first keeping unique sentences where we have more detected companies  \n",
    "    dicty = {}\n",
    "    for item in data.prompt_prep:\n",
    "        for sub_item in item:\n",
    "            key = sub_item[0]\n",
    "            value = sub_item[1]\n",
    "    \n",
    "            if key in dicty:\n",
    "                dicty[key].append(value)\n",
    "            else:\n",
    "                dicty[key] = [value] \n",
    "    for key in dicty.keys():\n",
    "        values = dicty[key]\n",
    "        flat_values = [item for sublist in values for item in sublist]\n",
    "        dicty[key] = flat_values\n",
    "\n",
    "    prompts_list = []\n",
    "    for key in dicty.keys():\n",
    "        for tuple in dicty[key]:\n",
    "            if tuple != []:\n",
    "                return_value = (key, tuple[1], tuple[0])\n",
    "                prompts_list.append(return_value)\n",
    "    \n",
    "    return prompts_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49cd5679-63c2-407a-9083-04d94ae29b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop (path, folder):\n",
    "    print(path)\n",
    "    data = pd.read_csv(path, \n",
    "                       sep = \";\",\n",
    "                       encoding='utf-8',\n",
    "                       usecols=['title','source','doc_id', 'date', 'sentences'],\n",
    "                       on_bad_lines = 'skip',\n",
    "                       engine = 'pyarrow')        \n",
    "    \n",
    "    initial_company = folder\n",
    "    \n",
    "    result = explode_companies(initial_company,data)\n",
    "\n",
    "    prompts_list = format_companies_explosion(result)\n",
    "    print(prompts_list)\n",
    "    with open('data/'+folder+'/'+folder+'_prompt_ready.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['text', 'initial_company', 'new_company'])  # Write header\n",
    "        writer.writerows(prompts_list) \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "363ede01-d3a8-4a60-a23c-33896525c99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Rolled__Ribbon_Battery/Rolled__Ribbon_Battery_0_news.csv\n",
      "Column 'doc_id' in include_columns does not exist in CSV file\n",
      "data/Tenergy/Tenergy_0_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/Morgan_Advanced_Materials/Morgan_Advanced_Materials_1000_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/Nanoramic_Laboratories/Nanoramic_Laboratories_0_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/American_Battery_Factory/American_Battery_Factory_100_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/Calogy_Solutions/Calogy_Solutions_0_news.csv\n",
      "Column 'doc_id' in include_columns does not exist in CSV file\n",
      "data/Trojan_Battery/Trojan_Battery_0_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/Enersys/Enersys_500_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/BMZ_USA/BMZ_USA_0_news.csv\n",
      "Column 'doc_id' in include_columns does not exist in CSV file\n",
      "data/LION_Electric/LION_Electric_700_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/Amphenol/Amphenol_350_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/Blue_Line_Battery/Blue_Line_Battery_0_news.csv\n",
      "Column 'doc_id' in include_columns does not exist in CSV file\n",
      "data/SK_Battery_America/SK_Battery_America_100_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/Battery_Specialties/Battery_Specialties_0_news.csv\n",
      "0\n",
      "name 'sentences' is not defined\n",
      "data/LI__CAP_Technologies/LI__CAP_Technologies_0_news.csv\n",
      "0\n",
      "LI__CAP_Technologies [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] 0 \n",
      " \n",
      "\n",
      "[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through all subfolders in the data folder\n",
    "data_folder_path ='data/' \n",
    "for subfolder in os.listdir(data_folder_path):\n",
    "    subfolder_path = os.path.join(data_folder_path, subfolder)\n",
    "    \n",
    "    # Check if it is a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Find the CSV file ending with ews.csv in the current subfolder\n",
    "        csv_files = glob.glob(os.path.join(subfolder_path, '*ews.csv'))\n",
    "        \n",
    "        # There should be exactly one CSV file per subfolder as per the given structure\n",
    "        if csv_files:\n",
    "            csv_file_path = csv_files[0]  # Get the path of the CSV file\n",
    "            subfolder_name = os.path.basename(subfolder_path)  # Get the name of the subfolder\n",
    "            initial_company = subfolder_name.replace('_', '')\n",
    "            # Print the subfolder name and the first few rows of the dataframe as an example\n",
    "            try:\n",
    "                run_loop(path = csv_file_path, \n",
    "                         folder = subfolder_name)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f28bb-028a-4a48-934a-e1abdecb103f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
