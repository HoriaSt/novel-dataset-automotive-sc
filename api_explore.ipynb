{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexis Nexis API for Data Extraction\n",
    "This notebook accesses the LexisNexis API using the lexisnexisapi Python package. Moreover, it tokenizes the articles into sentences and parses them as CSV\n",
    "\n",
    "Some useful links for querry development:\n",
    "- https://solutions.nexis.com/wsapi/news-and-directories/news/\n",
    "- https://www.lexisnexis.com/pdf/lexis-advance/terms-and-connectors.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to be used below\n",
    "The query elicitation was explained in the written master thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' W/25 (supply OR acquire OR support OR buy OR invest OR procure OR provide OR source OR deliver OR purchase OR obtain OR order OR request)  AND ( (\"car\" OR ALLCAPS(!EV) OR \"e-vehicle\") BUT NOT scooter BUT NOT bike) AND (battery OR \"storage\" OR \"lithium ion\" OR \"cell\" OR  \"modpack\" OR \"mod pack\" OR)'    \n",
    "\n",
    "#query for filtering the indexed information of LexisNexis through the API call function\n",
    "filter_date_eng = \"(year(Date) eq 2024 or year(Date) eq 2023 or year(Date) eq 2022 or year(Date) eq 2021 or year(Date) eq 2020 or year(Date) eq 2019) and Language eq LexisNexis.ServicesApi.Language'English'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import bs4 as bs\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from lexisnexisapi import webservices as ws\n",
    "from lexisnexisapi import credentials as cred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the NAATBatt list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading in the data for the NAATBAtt list - data can be downloaded in excel from the org's website\n",
    "\n",
    "file_path = 'naatbatt_sc.xlsx'\n",
    "\n",
    "# sheets in scope of the thesis - ModPack and Electrodes and Cells\n",
    "sheet_name_1 = '5-ModPack' \n",
    "sheet_name_2 = '4-Electrodes and Cells'\n",
    "\n",
    "# reading in the two sheets\n",
    "excel_1 = pd.read_excel(file_path, sheet_name=sheet_name_1)\n",
    "excel_2 = pd.read_excel(file_path, sheet_name=sheet_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Supply Chain Segment</th>\n",
       "      <th>Company</th>\n",
       "      <th>NAATBatt Member</th>\n",
       "      <th>Facility Name</th>\n",
       "      <th>Product Type</th>\n",
       "      <th>Product</th>\n",
       "      <th>Facility or Company Website</th>\n",
       "      <th>Facility Address</th>\n",
       "      <th>...</th>\n",
       "      <th>HQ Company</th>\n",
       "      <th>HQ Website</th>\n",
       "      <th>HQ City</th>\n",
       "      <th>HQ State or Province</th>\n",
       "      <th>HQ Country</th>\n",
       "      <th>QC</th>\n",
       "      <th>QC Date</th>\n",
       "      <th>Sources</th>\n",
       "      <th>Notes</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>C</td>\n",
       "      <td>Downstream</td>\n",
       "      <td>3M</td>\n",
       "      <td>No</td>\n",
       "      <td>3M Guin</td>\n",
       "      <td>Non-cell Components</td>\n",
       "      <td>Thermal Systems</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>6675 US Highway 43</td>\n",
       "      <td>...</td>\n",
       "      <td>3M</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>MN</td>\n",
       "      <td>US</td>\n",
       "      <td>EW</td>\n",
       "      <td>2021-08-09 00:00:00</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>Thermal Pads &amp; Fillers</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001</td>\n",
       "      <td>C</td>\n",
       "      <td>Downstream</td>\n",
       "      <td>3M</td>\n",
       "      <td>No</td>\n",
       "      <td>3M Cottage Grove</td>\n",
       "      <td>Non-cell Components</td>\n",
       "      <td>Thermal Systems</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>10746 Innovation Road</td>\n",
       "      <td>...</td>\n",
       "      <td>3M</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>MN</td>\n",
       "      <td>US</td>\n",
       "      <td>EW</td>\n",
       "      <td>2021-08-09 00:00:00</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>Thermal Pads &amp; Fillers</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5002</td>\n",
       "      <td>C</td>\n",
       "      <td>Downstream</td>\n",
       "      <td>3M</td>\n",
       "      <td>No</td>\n",
       "      <td>3M Springfield</td>\n",
       "      <td>Non-cell Components</td>\n",
       "      <td>Safety Systems</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>3211 E Chestnut Expressway</td>\n",
       "      <td>...</td>\n",
       "      <td>3M</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>MN</td>\n",
       "      <td>US</td>\n",
       "      <td>EW</td>\n",
       "      <td>2021-08-09 00:00:00</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>Rapid thermal suppression</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5003</td>\n",
       "      <td>C</td>\n",
       "      <td>Downstream</td>\n",
       "      <td>AA Portable Power Corp.</td>\n",
       "      <td>No</td>\n",
       "      <td>AA Portable Power Corp.</td>\n",
       "      <td>Cell Assemblies/ Groupings</td>\n",
       "      <td>Packs</td>\n",
       "      <td>aaportablepower.com</td>\n",
       "      <td>825 South 19th Street</td>\n",
       "      <td>...</td>\n",
       "      <td>AA Portable Power Corp.</td>\n",
       "      <td>batteryspace.com</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>CA</td>\n",
       "      <td>US</td>\n",
       "      <td>EW</td>\n",
       "      <td>2021-08-09 00:00:00</td>\n",
       "      <td>Thomasnet: Battery Packs / manufacturers; batt...</td>\n",
       "      <td>Mostly a battery distributor, but has producti...</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5004</td>\n",
       "      <td>C</td>\n",
       "      <td>Downstream</td>\n",
       "      <td>ADA Technologies, Inc.</td>\n",
       "      <td>No</td>\n",
       "      <td>ADA Technologies, Inc.</td>\n",
       "      <td>Non-cell Components</td>\n",
       "      <td>Thermal Systems</td>\n",
       "      <td>adatech.com</td>\n",
       "      <td>11149 Bradford Road</td>\n",
       "      <td>...</td>\n",
       "      <td>ADA Technolgies, Inc.</td>\n",
       "      <td>adatech.com</td>\n",
       "      <td>Littleton</td>\n",
       "      <td>CO</td>\n",
       "      <td>US</td>\n",
       "      <td>VP</td>\n",
       "      <td>2022-08-01 00:00:00</td>\n",
       "      <td>adatech.com; Questionnaire; https://www.buzzfi...</td>\n",
       "      <td>Thermal Interface Materials;</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID Status Supply Chain Segment                  Company NAATBatt Member  \\\n",
       "0  5000      C           Downstream                       3M              No   \n",
       "1  5001      C           Downstream                       3M              No   \n",
       "2  5002      C           Downstream                       3M              No   \n",
       "3  5003      C           Downstream  AA Portable Power Corp.              No   \n",
       "4  5004      C           Downstream   ADA Technologies, Inc.              No   \n",
       "\n",
       "             Facility Name                Product Type          Product  \\\n",
       "0                  3M Guin         Non-cell Components  Thermal Systems   \n",
       "1         3M Cottage Grove         Non-cell Components  Thermal Systems   \n",
       "2           3M Springfield         Non-cell Components   Safety Systems   \n",
       "3  AA Portable Power Corp.  Cell Assemblies/ Groupings            Packs   \n",
       "4   ADA Technologies, Inc.         Non-cell Components  Thermal Systems   \n",
       "\n",
       "  Facility or Company Website            Facility Address  ...  \\\n",
       "0                      3m.com          6675 US Highway 43  ...   \n",
       "1                      3m.com       10746 Innovation Road  ...   \n",
       "2                      3m.com  3211 E Chestnut Expressway  ...   \n",
       "3         aaportablepower.com       825 South 19th Street  ...   \n",
       "4                 adatech.com         11149 Bradford Road  ...   \n",
       "\n",
       "                HQ Company        HQ Website     HQ City HQ State or Province  \\\n",
       "0                       3M            3m.com  Saint Paul                   MN   \n",
       "1                       3M            3m.com  Saint Paul                   MN   \n",
       "2                       3M            3m.com  Saint Paul                   MN   \n",
       "3  AA Portable Power Corp.  batteryspace.com    Richmond                   CA   \n",
       "4    ADA Technolgies, Inc.       adatech.com   Littleton                   CO   \n",
       "\n",
       "  HQ Country  QC              QC Date  \\\n",
       "0         US  EW  2021-08-09 00:00:00   \n",
       "1         US  EW  2021-08-09 00:00:00   \n",
       "2         US  EW  2021-08-09 00:00:00   \n",
       "3         US  EW  2021-08-09 00:00:00   \n",
       "4         US  VP  2022-08-01 00:00:00   \n",
       "\n",
       "                                             Sources  \\\n",
       "0                                             3m.com   \n",
       "1                                             3m.com   \n",
       "2                                             3m.com   \n",
       "3  Thomasnet: Battery Packs / manufacturers; batt...   \n",
       "4  adatech.com; Questionnaire; https://www.buzzfi...   \n",
       "\n",
       "                                               Notes     file  \n",
       "0                             Thermal Pads & Fillers  ModPack  \n",
       "1                             Thermal Pads & Fillers  ModPack  \n",
       "2                          Rapid thermal suppression  ModPack  \n",
       "3  Mostly a battery distributor, but has producti...  ModPack  \n",
       "4                      Thermal Interface Materials;   ModPack  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_1['file'] = 'ModPack'\n",
    "excel_2['file'] = 'ElectrodesCells'\n",
    "\n",
    "excel = pd.concat([excel_1, excel_2], axis = 0)\n",
    "excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4128484/1645062754.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Inc\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\"3M\", \"ALLCAPS(3M)\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Inc \",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\".\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\",\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Corp\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Company\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" LLC\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Ltd\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" (JV of Ford and SK On)\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Co\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" JV\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" SDI\",\"\"))\n",
      "/tmp/ipykernel_4128484/1645062754.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\"-\",'\" AND \"'))\n",
      "/tmp/ipykernel_4128484/1645062754.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.rstrip(' '))\n",
      "/tmp/ipykernel_4128484/1645062754.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.Company = companies_search.Company.apply(lambda x: x.lstrip(' '))\n"
     ]
    }
   ],
   "source": [
    "companies_search = excel[['Company', 'Product', 'Product Type', 'file']]\n",
    "\n",
    "#removing noise from the company names\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Inc\",\"\"))\n",
    "#if we would not enforce ALLCAPS for 3M, hits for other subjects would return - for e.g. 3m as an abbreviation for 3 million \n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\"3M\", \"ALLCAPS(3M)\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Inc \",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\".\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\",\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Corp\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Company\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" LLC\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Ltd\",\"\"))\n",
    "#stand for Joint Venture of Ford and SK On - the name of the joint venture is enough to use in the query\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" (JV of Ford and SK On)\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" Co\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" JV\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\" SDI\",\"\"))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.replace(\"-\",'\" AND \"'))\n",
    "#removing empty spaces from the beginning and the end of the names\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.rstrip(' '))\n",
    "companies_search.Company = companies_search.Company.apply(lambda x: x.lstrip(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4128484/2380239521.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  companies_search.drop_duplicates(subset = ['Company', 'file'], keep = 'first', ignore_index = True, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "#this drops rows when one company has more facilities - if it has more facilities, it appears multiple times \n",
    "companies_search.drop_duplicates(subset = ['Company', 'file'], keep = 'first', ignore_index = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product Type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALLCAPS(3M)</td>\n",
       "      <td>Thermal Systems</td>\n",
       "      <td>Non-cell Components</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA Portable Power</td>\n",
       "      <td>Packs</td>\n",
       "      <td>Cell Assemblies/ Groupings</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA Technologies</td>\n",
       "      <td>Thermal Systems</td>\n",
       "      <td>Non-cell Components</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AKASOL</td>\n",
       "      <td>Packs</td>\n",
       "      <td>Cell Assemblies/ Groupings</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alion Science and Technology</td>\n",
       "      <td>Modules/ Arrays</td>\n",
       "      <td>Cell Assemblies/ Groupings</td>\n",
       "      <td>ModPack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Ultium Cells</td>\n",
       "      <td>NMC</td>\n",
       "      <td>Prismatic (pouch) cells</td>\n",
       "      <td>ElectrodesCells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>VinFast</td>\n",
       "      <td>Other/ Unknown</td>\n",
       "      <td>Other/ Unknown cells</td>\n",
       "      <td>ElectrodesCells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ElectrodesCells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Xalt Energy MI</td>\n",
       "      <td>NMC</td>\n",
       "      <td>Prismatic (pouch) cells</td>\n",
       "      <td>ElectrodesCells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Xerion Advanced Battery</td>\n",
       "      <td>LCO</td>\n",
       "      <td>Cylindrical cells</td>\n",
       "      <td>ElectrodesCells</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Company          Product  \\\n",
       "0                     ALLCAPS(3M)  Thermal Systems   \n",
       "1               AA Portable Power            Packs   \n",
       "2                ADA Technologies  Thermal Systems   \n",
       "3                          AKASOL            Packs   \n",
       "4    Alion Science and Technology  Modules/ Arrays   \n",
       "..                            ...              ...   \n",
       "175                  Ultium Cells              NMC   \n",
       "176                       VinFast   Other/ Unknown   \n",
       "177                    Volkswagen              NaN   \n",
       "178                Xalt Energy MI              NMC   \n",
       "179       Xerion Advanced Battery              LCO   \n",
       "\n",
       "                   Product Type             file  \n",
       "0           Non-cell Components          ModPack  \n",
       "1    Cell Assemblies/ Groupings          ModPack  \n",
       "2           Non-cell Components          ModPack  \n",
       "3    Cell Assemblies/ Groupings          ModPack  \n",
       "4    Cell Assemblies/ Groupings          ModPack  \n",
       "..                          ...              ...  \n",
       "175     Prismatic (pouch) cells  ElectrodesCells  \n",
       "176        Other/ Unknown cells  ElectrodesCells  \n",
       "177                         NaN  ElectrodesCells  \n",
       "178     Prismatic (pouch) cells  ElectrodesCells  \n",
       "179           Cylindrical cells  ElectrodesCells  \n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(companies_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the News API\n",
    "#### Functions to be used for accessing the API and Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_text(data):\n",
    "    '''\n",
    "        This function receives an XML structure and extracts from it the title of an article,\n",
    "        the full text, the document id and the date the news was published. \n",
    "        The result is then returned in a dictionary.\n",
    "    '''\n",
    "    #accessing the XML using beautiful soup and its html parser\n",
    "    file = bs.BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    #the body of the API response is nested in this path\n",
    "    text = file.content.articledoc.bodytext\n",
    "    \n",
    "    #getting the full_text\n",
    "    #text is split by rows in the article so needs to be appended in one string\n",
    "    full_text = ''\n",
    "    for paragraph in text:\n",
    "        full_text = full_text + paragraph.text\n",
    "    \n",
    "    #saving some metadata in case we want to filter later on\n",
    "    title = file.title.text\n",
    "    entry_id = file.id.text\n",
    "    published = file.published.text\n",
    "    \n",
    "    #making sure the CSVs don't get broken by bad characters\n",
    "    full_text = full_text.replace(';', '').replace('\\n','')\n",
    "    title = title.replace(';', '')\n",
    "    \n",
    "    res = {\n",
    "        'title':title,\n",
    "        'text':full_text,\n",
    "        'doc_id':entry_id,\n",
    "        'date':published\n",
    "    }\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iterate_results (data_from_request):\n",
    "    '''\n",
    "        This function iterates through the up to 50 articles received in one batch \n",
    "        and returns them as rows in a dataframe. It takes as input the raw request response\n",
    "        The function extracts the title, text and source name for one article \n",
    "        and adds it to a pd.df which is then returned.\n",
    "    '''\n",
    "\n",
    "    #creating empty df\n",
    "    result = pd.DataFrame(columns=['title', 'text', 'source'])\n",
    "\n",
    "    #getting the dictionary of 50 articles\n",
    "    texts = data_from_request['value']\n",
    "\n",
    "    for text in texts:\n",
    "        try:\n",
    "            content = text['Document']['Content'] #this is the full text\n",
    "            \n",
    "            #the title and the content have to be extracted from the XML format\n",
    "            res = access_text(content)\n",
    "            res['source'] = text['Source']['Name']#only the source name can be directly accessed\n",
    "            \n",
    "            #converting to a dataframe for returning the result\n",
    "            res = pd.DataFrame(res, index=[0])\n",
    "\n",
    "            #concatenating the results during the iteration\n",
    "            result = pd.concat([result, res], ignore_index=True)\n",
    "\n",
    "        #error handling for possible issues\n",
    "        except TypeError as e:\n",
    "            print(f\"TypeError occurred: {e} - Skipping this text.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError occurred: {e} - Skipping this text.\")\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def take_sentences (company = str, \n",
    "                    number = int, \n",
    "                    text = str):\n",
    "    '''\n",
    "        This function takes as input a company name,\n",
    "        a number corresponding to how many sentences should be extracted around the identified\n",
    "        company and a text to work with. \n",
    "        The function tokenizes the text into sentences and performs a search for the\n",
    "        company name from the NAATBatt list. A number of sentences will be extracted around\n",
    "        the company name. The final result to be returned is a json dictionary containing a unique\n",
    "        identifier of the match as key and the extracted sentences as values.\n",
    "    '''\n",
    "    #tokenizing the sentences using the nltk tokenizer imported in the first command\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    #searching the company name in the lowercase\n",
    "    #searching through lowercase chars so allcaps don't influence the result\n",
    "    company_lower = company.lower()\n",
    "    indices = [i for i, sentence in enumerate(sentences) if company_lower in sentence.lower()]\n",
    "    \n",
    "    result = {}\n",
    "    for index in indices:\n",
    "\n",
    "        # get the last 3(number) sentences before and the next 3(number) sentences after the sentence with the name\n",
    "        start = max(0, index - number)\n",
    "        end = min(len(sentences), index + number + 1)\n",
    "\n",
    "        # extract the required sentences\n",
    "        surrounding_sentences = sentences[start:end]\n",
    "        surrounding_sentences = ' '.join(surrounding_sentences)\n",
    "        #replacing a weird character that could harm the CSV parsing\n",
    "        surrounding_sentences = surrounding_sentences.replace('•', ' ')\n",
    "\n",
    "        #key = index, unique identifeir and value = surrounding_sentences\n",
    "        result [index] = surrounding_sentences\n",
    "    \n",
    "    result = json.dumps(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_res (res, company, no_artilces = int):  \n",
    "    '''\n",
    "        This function takes as input a company name and the articles returned for this company\n",
    "        as well as the number of articles retrieved for the company\n",
    "        It saves the articles in a csv file under a data folder\n",
    "    '''\n",
    "    # cleaning the company name so it can be used as a file path\n",
    "    company = company.replace(' ', '_').replace('\"','').replace(\"AND\", \"\").replace(\"(\",\"\").replace(\")\",\"\").replace('ALLCAPS',\"\")\n",
    "    \n",
    "    # get the correct path syntax for the system in the current wd\n",
    "    path = os.path.join(os.getcwd(), 'data/'+company)\n",
    "\n",
    "    # check if the directory already exists\n",
    "    if not os.path.exists(path):\n",
    "        # create the directory if it does not\n",
    "        os.makedirs(path)\n",
    "\n",
    "    #create the file name and path\n",
    "    file_name = company+'_'+str(no_artilces)+'_news.csv'\n",
    "    address = path + '/' + file_name\n",
    "\n",
    "    #saving the pandas df to the path with ; as separator\n",
    "    res.to_csv(path_or_buf = address, sep = ';')\n",
    "    \n",
    "    return\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_batchnews_need (search_query):\n",
    "    '''\n",
    "        This function takes as input a search query and sends a dummy request\n",
    "        to the LexisNexis API to obtain the total number of articles for a company. \n",
    "        This is required to evaluate if the BatchNews API or the News API must be used, as indicated \n",
    "        by the team at LexisNexis. If there are more than 500 articles to be returned for one query,\n",
    "        than we must use the BatchNews API since it is more efficient on their side.\n",
    "        The function returns the endpoint to be used and the number of articles identified\n",
    "    '''\n",
    "    #filtering the articles by date\n",
    "    parameters ={\n",
    "            '$search':search_query,\n",
    "            '$filter':filter_date_eng,\n",
    "            '$top': 0, #we don't need any articles now, just the response header\n",
    "            '$skip':0 \n",
    "    }\n",
    "\n",
    "    #api call using the lexisnexisapi for Python\n",
    "    data = ws.call_api(access_token = token,\n",
    "                       endpoint = 'News', \n",
    "                       params = parameters)\n",
    "\n",
    "    #extracting the number of articles\n",
    "    count = data['@odata.count']\n",
    "\n",
    "    #checking wich endpoint to use\n",
    "    if count > 500:\n",
    "        endpoint = 'BatchNews'\n",
    "        return endpoint, count\n",
    "    else: \n",
    "        endpoint = 'News'\n",
    "        return endpoint, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Code for Querrying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create an access token\n",
    "#enter Client Id, Client Secret Here - to be obtained from WU library\n",
    "token = ws.token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xerion Advanced Battery News 14 \n",
      " Xerion Advanced Battery W/25 (supply OR acquire OR support OR buy OR invest OR procure OR provide OR source OR deliver OR purchase OR obtain OR order OR request)  AND ( (\"car\" OR ALLCAPS(!EV) OR \"e-vehicle\") BUT NOT scooter BUT NOT bike) AND (battery OR \"storage\" OR \"lithium ion\" OR \"cell\" OR  \"modpack\" OR \"mod pack\" OR)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_company = \"Xerion Advanced Battery\" #for continuing from one particular company in the NAATBatt list, insert the company name here\n",
    "#we can start from a specified copmany in case an error appeared and we need to restart the downloading from the previously failed point\n",
    "start_index  = companies_search[companies_search.Company == start_company].index[0]\n",
    "\n",
    "#iterating through the NAATBatt list of companies\n",
    "for company in companies_search.Company[start_index:]:\n",
    "    #cleaning the company name for logging\n",
    "    company_clean = company.replace('\"','').replace(\"AND\", \"\").replace(\"(\",\"\").replace(\")\",\"\").replace('ALLCAPS',\"\").replace('_','')\n",
    "\n",
    "    #appending the company name to the search query from the notebook beginning\n",
    "    search_query = company + query \n",
    "\n",
    "    #filtering the date, english texts and returning 50 articles at a time (max number of articles to be returned at once)\n",
    "    parameters ={\n",
    "            '$search':search_query,\n",
    "            '$expand':'Document', #A navigation property name which will be included with the current result set.\n",
    "            '$filter':filter_date_eng,\n",
    "            '$top': 50,\n",
    "            '$skip':0\n",
    "    }\n",
    "    \n",
    "    #checking if we need to use News or BatchNews\n",
    "    endpoint, count = check_batchnews_need(search_query)\n",
    "    \n",
    "    #some primitive logging\n",
    "    print(company_clean, endpoint, count, '\\n', search_query)\n",
    "\n",
    "    #actually calling the API\n",
    "    data = ws.call_api(access_token = token,\n",
    "                       endpoint = endpoint, \n",
    "                       params = parameters)\n",
    "            \n",
    "    #opening the XML returned\n",
    "    final_res = iterate_results(data)\n",
    "    \n",
    "    #transforming the text into a dictionary with only 2 sentences around the key company\n",
    "    final_res['sentences'] = final_res.text.apply(lambda x: take_sentences(company_clean, 1, x) )\n",
    "    \n",
    "    #checking the pagination of the response\n",
    "    #if more than 50 articles are possible to extract, a link to the next ones appears\n",
    "    if '@odata.nextLink' in data.keys():\n",
    "        skip = data['@odata.nextLink'].split('$skip=')[1][:2]\n",
    "    else: skip =0\n",
    "\n",
    "    #looping through the pagination - basically sending repeated requests with a wait of 20s for getting all articles\n",
    "    #the wait is needed to not breach the throttle limits\n",
    "    while '@odata.nextLink' in data.keys() and int(skip) <= count:\n",
    "\n",
    "        #adding the skip to the list of parameters so we don't always get the same articles\n",
    "        parameters['$skip'] = skip\n",
    "        \n",
    "        data = ws.call_api(access_token = token,\n",
    "                           endpoint = endpoint, \n",
    "                           params = parameters)  #Set endpoint='News'\n",
    "\n",
    "        ##opening the XML returned\n",
    "        res = iterate_results(data)\n",
    "\n",
    "        #transforming the text into a dictionary with only 2 sentences around the key company\n",
    "        res['sentences'] = res.text.apply(lambda x: take_sentences(company_clean, 1, x) )\n",
    "        \n",
    "        #concatenating the dataframes for having all articles in a single CSV\n",
    "        frames = [final_res, res]\n",
    "        final_res = pd.concat(frames, \n",
    "                              axis = 0, \n",
    "                              ignore_index = True)\n",
    "        \n",
    "        #moving on to the next 50 articles\n",
    "        skip = int(skip) + 50\n",
    "\n",
    "        #primitve logging again\n",
    "        print(skip)\n",
    "\n",
    "        # Pause the execution for 20 seconds\n",
    "        time.sleep(20)\n",
    "\n",
    "    #after all articles are returned, saving them to CSV\n",
    "    save_res(final_res, company, skip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
